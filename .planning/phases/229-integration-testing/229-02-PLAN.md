---
phase: 229-integration-testing
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - packages/daemon/src/services/incoming/__tests__/integration-resilience.test.ts
autonomous: true
requirements:
  - M-03
  - M-07

must_haves:
  truths:
    - "50-block gap simulation triggers gap recovery that recovers all missed transactions"
    - "KillSwitch in SUSPENDED state: DB records are written, EventBus events fire, but NotificationService.notify is NOT called"
    - "50 dust transactions flushed through the monitor service produce at most 1 notification (cooldown suppresses the rest)"
    - "createGapRecoveryHandler factory (tested in isolation) calls pollAll() on the correct subscriber and routes recovered TXs through the queue — production IncomingTxMonitorService uses a console.debug stub, so E2E gap recovery is not tested through the monitor service"
    - "Cooldown expiry allows notifications to resume after the configured cooldown period"
  artifacts:
    - path: "packages/daemon/src/services/incoming/__tests__/integration-resilience.test.ts"
      provides: "Integration tests for gap recovery, KillSwitch suppression, notification cooldown"
      min_lines: 200
  key_links:
    - from: "IncomingTxMonitorService flush handler"
      to: "KillSwitchService.getState()"
      via: "notification suppression check"
      pattern: "killState.*ACTIVE"
    - from: "IncomingTxMonitorService flush handler"
      to: "notifyCooldown Map"
      via: "per-wallet per-event-type cooldown tracking"
      pattern: "isCooldownActive"
    - from: "SubscriptionMultiplexer reconnect"
      to: "onGapRecovery callback"
      via: "state transition from non-WS_ACTIVE to WS_ACTIVE"
      pattern: "onGapRecovery"
---

<objective>
Create integration tests verifying gap recovery after connection loss, KillSwitch notification suppression (M-03), and notification cooldown under dust flood (M-07).

Purpose: These tests validate the security and resilience properties of the incoming TX pipeline -- the behaviors that protect operators from alert fatigue and ensure data integrity even when the system is locked down.

Output: `packages/daemon/src/services/incoming/__tests__/integration-resilience.test.ts` with ~12-15 test cases.
</objective>

<execution_context>
@/Users/minho.yoo/.claude/get-shit-done/workflows/execute-plan.md
@/Users/minho.yoo/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Implementation files
@packages/daemon/src/services/incoming/incoming-tx-queue.ts
@packages/daemon/src/services/incoming/subscription-multiplexer.ts
@packages/daemon/src/services/incoming/incoming-tx-monitor-service.ts
@packages/daemon/src/services/incoming/incoming-tx-workers.ts
@packages/daemon/src/services/incoming/safety-rules.ts
@packages/core/src/interfaces/connection-state.ts
@packages/core/src/interfaces/IChainSubscriber.ts
@packages/core/src/interfaces/chain-subscriber.types.ts

# Existing test patterns
@packages/daemon/src/services/incoming/__tests__/incoming-tx-monitor-service.test.ts
@packages/daemon/src/services/incoming/__tests__/incoming-tx-workers.test.ts
@packages/daemon/src/services/incoming/__tests__/safety-rules.test.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create integration-resilience.test.ts with gap recovery and KillSwitch tests</name>
  <files>packages/daemon/src/services/incoming/__tests__/integration-resilience.test.ts</files>
  <action>
Create the test file following existing vitest patterns. Reuse the same mock helper patterns from `incoming-tx-monitor-service.test.ts` (createMockSqlite, createMockEventBus, createMockKillSwitch, createMockNotificationService, etc.).

**Mock Strategy:** Use the real IncomingTxMonitorService with a real IncomingTxQueue, but mock all external deps (SQLite, EventBus, KillSwitch, NotificationService, BackgroundWorkers, SubscriberFactory). Access internal queue and flush handler via `(service as any)` where needed.

**Section 1: Gap Recovery Integration (3-4 tests)**

Test that the SubscriptionMultiplexer + reconnectLoop + gap recovery callback works end-to-end:

1. Create SubscriptionMultiplexer with a mock subscriber that resolves waitForDisconnect() on demand (via _triggerDisconnect helper). Create onGapRecovery mock callback. Add a wallet, then trigger disconnect. Wait for reconnectLoop to fire. Verify: onGapRecovery is called with the correct chain, network, and walletIds. Use fast reconnect config (initialDelayMs: 10, maxDelayMs: 20, jitterFactor: 0).
2. Test createGapRecoveryHandler factory in isolation (imported from incoming-tx-workers.ts). Create it with a mock subscribers Map containing a subscriber with a mock pollAll(). Call the handler. Verify: pollAll() is called once. NOTE: Production IncomingTxMonitorService uses a console.debug stub for onGapRecovery, not the real createGapRecoveryHandler. This test verifies the handler factory itself works correctly when wired up, as the wiring will be completed in a future phase.
3. Simulate a 50-block gap via the handler factory: create a createGapRecoveryHandler with a subscriber whose pollAll() pushes 50 mock IncomingTransactions into a shared IncomingTxQueue (via the onTransaction callback). Call the handler, then flush the queue to a mock DB. Verify: all 50 transactions are in the DB (50 stmt.run calls). This tests handler factory -> queue -> flush -> DB path.

**Section 2: KillSwitch Notification Suppression - M-03 (4-5 tests)**

**IMPORTANT — Per-test cooldown isolation:** KillSwitch tests share the same IncomingTxMonitorService class which has an internal `notifyCooldown` Map. Cooldown state MUST NOT bleed between tests. Use one of these strategies:
- **Preferred:** Create a fresh IncomingTxMonitorService instance in each test (not shared via `beforeAll`).
- **Alternative:** In `beforeEach`, clear the cooldown map via `(service as any).notifyCooldown.clear()` to reset state.
Either approach ensures that a test checking "ACTIVE sends notification" is not affected by a prior test that already populated the cooldown for the same wallet+eventType.

Test that KillSwitch state controls notification delivery while preserving data integrity:

1. KillSwitch ACTIVE: Create a fresh IncomingTxMonitorService with killSwitch.getState() returning {state:'ACTIVE'}. Push 3 transactions to internal queue. Execute flush handler. Verify: 3 eventBus.emit('transaction:incoming') calls AND 3 notificationService.notify calls (first one only, others may be cooldown-suppressed -- so verify at least 1 notify call).
2. KillSwitch SUSPENDED: Create a fresh service instance with killSwitch returning {state:'SUSPENDED'}. Push 3 transactions. Execute flush handler. Verify: 3 eventBus.emit('transaction:incoming') calls (events always fire), 0 notificationService.notify calls.
3. KillSwitch LOCKED: Fresh service instance with {state:'LOCKED'}. Verify same behavior: events fire, no notifications.
4. KillSwitch SUSPENDED -> DB writes: Fresh service instance. Push a suspicious transaction (unknown token, mock token_registry query returns undefined). Execute flush handler. Verify: DB UPDATE is_suspicious=1 is called (DB writes persist), eventBus.emit('transaction:incoming:suspicious') fires, but notificationService.notify is NOT called.
5. KillSwitch transitions: Fresh service instance. Execute flush with ACTIVE (notification sent), then change mock to SUSPENDED, clear cooldown map via `(service as any).notifyCooldown.clear()`, execute another flush (notification suppressed). Verify the transition is respected — the second flush produces events but no notifications.

Each test name: `it('M-03: [behavior]', ...)`.
  </action>
  <verify>
Run: `cd /Users/minho.yoo/dev/wallet/WAIaaS && pnpm vitest run packages/daemon/src/services/incoming/__tests__/integration-resilience.test.ts --reporter=verbose`

All tests pass. Gap recovery and KillSwitch tests are green.
  </verify>
  <done>
7-9 tests pass covering gap recovery integration (3-4 tests) and KillSwitch notification suppression M-03 (4-5 tests). Tests verify cross-component data flow: subscriber -> queue -> flush -> event/notification with KillSwitch gating.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add notification cooldown / dust flood tests (M-07)</name>
  <files>packages/daemon/src/services/incoming/__tests__/integration-resilience.test.ts</files>
  <action>
Append to the same test file created in Task 1.

**Section 3: Notification Cooldown / Dust Flood - M-07 (5-6 tests)**

Test that rapid incoming transactions (dust flood scenario) are rate-limited by the per-wallet per-event-type cooldown mechanism:

1. **50 dust TX flood - at most 1 notification**: Create IncomingTxMonitorService with cooldownMinutes=5, KillSwitch ACTIVE. Push 50 unique transactions to the internal queue (different txHash, same walletId). Execute the flush handler once (will flush up to 100, so all 50 in one cycle). Verify: exactly 1 notificationService.notify call (first TX triggers notification, remaining 49 are cooldown-suppressed). All 50 eventBus.emit calls still fire. All 50 DB records inserted.

2. **Cooldown per wallet isolation**: Push transactions for wallet-A and wallet-B in the same batch. Verify: each wallet gets its own cooldown -- wallet-A gets 1 notification, wallet-B gets 1 notification (total 2 notify calls), even though they share the same event type.

3. **Cooldown per event type isolation**: Push a normal transaction and a suspicious transaction (different event types: TX_INCOMING vs TX_INCOMING_SUSPICIOUS) for the same wallet. Verify: 2 notify calls (one per event type), because cooldowns track wallet+eventType combination.

4. **Cooldown expiry**: Execute flush with one TX (notification sent). Manually manipulate the internal `notifyCooldown` Map via `(service as any).notifyCooldown` to set the last-notified timestamp to 6 minutes ago (past the 5-minute cooldown). Execute flush with another TX. Verify: second notification IS sent (cooldown expired). Total: 2 notify calls.

5. **Cooldown cleared on stop**: Execute flush (notification sent, cooldown recorded). Call service.stop(). Create a new flush handler after restart by calling service.start() again (or verify the cooldown Map is cleared). Verify: cooldown map size is 0 after stop.

6. **Multiple flush cycles within cooldown window**: Execute flush with 1 TX (notification sent). Push 10 more TXs. Execute flush again (within cooldown). Verify: no additional notifications on second flush. EventBus events still fire for all 10.

The key verification for success criteria #5 from ROADMAP: "50개 더스트 TX를 연속 전송해도 알림이 5개 이하로 제한된다" -- Test 1 verifies this directly (50 TX -> 1 notification, which is certainly <= 5).

Each test name: `it('M-07: [behavior]', ...)`.
  </action>
  <verify>
Run: `cd /Users/minho.yoo/dev/wallet/WAIaaS && pnpm vitest run packages/daemon/src/services/incoming/__tests__/integration-resilience.test.ts --reporter=verbose`

All tests pass including both Task 1 and Task 2 sections.
  </verify>
  <done>
12-15 total tests pass in integration-resilience.test.ts, covering gap recovery, KillSwitch suppression (M-03), and notification cooldown (M-07). The dust flood test specifically validates ROADMAP success criteria #5 (50 TX -> <=5 notifications).
  </done>
</task>

</tasks>

<verification>
```bash
cd /Users/minho.yoo/dev/wallet/WAIaaS && pnpm vitest run packages/daemon/src/services/incoming/__tests__/integration-resilience.test.ts --reporter=verbose
```

Expected: 12-15 tests pass, organized in 3 describe blocks (Gap Recovery, KillSwitch M-03, Notification Cooldown M-07). No failures.

Full suite validation:
```bash
cd /Users/minho.yoo/dev/wallet/WAIaaS && pnpm vitest run packages/daemon/src/services/incoming/__tests__/ --reporter=verbose
```

Expected: All tests pass (existing ~106 + new ~15-20 from plan 01 + new ~12-15 from plan 02 = ~130-140 total).
</verification>

<success_criteria>
1. integration-resilience.test.ts exists with 12-15 passing tests
2. Gap recovery tests verify: (a) SubscriptionMultiplexer reconnect triggers onGapRecovery callback, (b) createGapRecoveryHandler factory (tested in isolation) calls pollAll -> queue -> DB flow
3. KillSwitch tests verify DB writes persist + events fire but notifications are suppressed when SUSPENDED/LOCKED
4. Cooldown tests verify 50 dust TXs produce at most 1 notification per wallet+eventType
5. All prior tests remain passing
6. ROADMAP success criteria #4 (KillSwitch SUSPENDED -> DB yes, events fire, external notifications suppressed) and #5 (50 dust -> <=5 notifications) are directly verified
</success_criteria>

<output>
After completion, create `.planning/phases/229-integration-testing/229-02-SUMMARY.md`
</output>
