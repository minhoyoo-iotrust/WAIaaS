---
phase: 49-daemon-infra
plan: 03
type: tdd
wave: 2
depends_on: [49-01, 49-02]
files_modified:
  - packages/daemon/package.json
  - packages/daemon/src/infrastructure/config/loader.ts
  - packages/daemon/src/infrastructure/config/index.ts
  - packages/daemon/src/lifecycle/daemon.ts
  - packages/daemon/src/lifecycle/signal-handler.ts
  - packages/daemon/src/lifecycle/workers.ts
  - packages/daemon/src/lifecycle/index.ts
  - packages/daemon/src/index.ts
  - packages/daemon/src/__tests__/config-loader.test.ts
  - packages/daemon/src/__tests__/lifecycle.test.ts
autonomous: true

must_haves:
  truths:
    - "config.toml is parsed by smol-toml into 7 sections (daemon, keystore, database, rpc, notifications, security, walletconnect) and validated by a Zod ConfigSchema that enforces types, ranges, and defaults for all flat keys"
    - "Environment variable WAIAAS_{SECTION}_{KEY} overrides the corresponding toml value with priority env > toml > default"
    - "Nested TOML sections like [rpc.solana] are detected and rejected with an explicit error message"
    - "Daemon startup runs a 6-step sequence (env validation/config/flock -> DB init -> keystore unlock -> adapter init -> HTTP server -> workers/PID) with per-step timeouts and a 90-second overall cap"
    - "SIGTERM triggers a 10-step graceful shutdown ending with WAL checkpoint(TRUNCATE), keystore lock (sodium_memzero), DB close, and PID file deletion"
    - "flock exclusive lock on daemon.lock prevents a second daemon instance from starting"
    - "BackgroundWorkers run WAL checkpoint (5-min interval, PASSIVE) and expired session cleanup (1-min interval)"
  artifacts:
    - path: "packages/daemon/src/infrastructure/config/loader.ts"
      provides: "loadConfig() with smol-toml parsing, nested section detection, env override, Zod validation"
      exports: ["loadConfig", "DaemonConfigSchema"]
    - path: "packages/daemon/src/lifecycle/daemon.ts"
      provides: "DaemonLifecycle class with start() 6-step sequence and shutdown() 10-step cascade"
      exports: ["DaemonLifecycle"]
    - path: "packages/daemon/src/lifecycle/signal-handler.ts"
      provides: "registerSignalHandlers() for SIGINT/SIGTERM/SIGBREAK"
      exports: ["registerSignalHandlers"]
    - path: "packages/daemon/src/lifecycle/workers.ts"
      provides: "BackgroundWorkers class with register/startAll/stopAll"
      exports: ["BackgroundWorkers"]
    - path: "packages/daemon/src/__tests__/config-loader.test.ts"
      provides: "Tests for TOML parsing, env override priority, nested section rejection, Zod validation"
    - path: "packages/daemon/src/__tests__/lifecycle.test.ts"
      provides: "Tests for flock contention, PID file management, startup timeout, shutdown sequence, BackgroundWorkers"
  key_links:
    - from: "packages/daemon/src/infrastructure/config/loader.ts"
      to: "smol-toml + zod"
      via: "parse(tomlContent) -> detectNestedSections -> applyEnvOverrides -> ConfigSchema.parse"
      pattern: "parse.*smol-toml|ConfigSchema\\.parse"
    - from: "packages/daemon/src/lifecycle/daemon.ts"
      to: "packages/daemon/src/infrastructure/database/connection.ts"
      via: "createDatabase() in startup Step 2"
      pattern: "createDatabase"
    - from: "packages/daemon/src/lifecycle/daemon.ts"
      to: "packages/daemon/src/infrastructure/keystore/keystore.ts"
      via: "LocalKeyStore in startup Step 3"
      pattern: "LocalKeyStore"
    - from: "packages/daemon/src/lifecycle/daemon.ts"
      to: "flock lock file"
      via: "acquireDaemonLock() with exclusive non-blocking flock on daemon.lock"
      pattern: "acquireDaemonLock|flock|EWOULDBLOCK"
---

<objective>
config.toml loader with smol-toml parsing, env override, and Zod validation, plus daemon lifecycle (6-step startup, 10-step shutdown, flock, PID, BackgroundWorkers).

Purpose: The daemon needs to load configuration from `config.toml` with environment variable overrides, then orchestrate a deterministic startup and shutdown sequence. Startup must validate the environment, initialize DB, unlock keystore, start workers, and write PID -- with per-step timeouts and fail-fast/soft policies. Shutdown must gracefully release all resources, checkpoint WAL, zero key material, and clean up PID. Only one daemon instance may run at a time, enforced by flock.

Output: Config loader module, DaemonLifecycle class, signal handler, BackgroundWorkers, and comprehensive tests.
</objective>

<execution_context>
@/Users/minho.yoo/.claude/get-shit-done/workflows/execute-plan.md
@/Users/minho.yoo/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/49-daemon-infra/49-01-SUMMARY.md
@.planning/phases/49-daemon-infra/49-02-SUMMARY.md
@.planning/deliverables/24-monorepo-data-directory.md (SSoT for config.toml 7 sections, flat keys, env override pattern, Zod ConfigSchema)
@.planning/deliverables/28-daemon-lifecycle-cli.md (SSoT for 7-step startup, 10-step shutdown, flock, PID, BackgroundWorkers, signal handling, timeouts)
@packages/core/src/schemas/config.schema.ts (existing @waiaas/core ConfigSchema -- will be superseded by daemon's full DaemonConfigSchema)
@packages/daemon/src/infrastructure/database/connection.ts (from 49-01: createDatabase, closeDatabase)
@packages/daemon/src/infrastructure/keystore/keystore.ts (from 49-02: LocalKeyStore)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Config loader (smol-toml + Zod + env override) + DaemonLifecycle class + signal handler + BackgroundWorkers</name>
  <files>
    packages/daemon/package.json
    packages/daemon/src/infrastructure/config/loader.ts
    packages/daemon/src/infrastructure/config/index.ts
    packages/daemon/src/lifecycle/daemon.ts
    packages/daemon/src/lifecycle/signal-handler.ts
    packages/daemon/src/lifecycle/workers.ts
    packages/daemon/src/lifecycle/index.ts
    packages/daemon/src/index.ts
  </files>
  <action>
**1. Install config dependency** (add to packages/daemon/package.json if not present):

Production dependency:
- `smol-toml` ^1.3.0

Run `pnpm install` from workspace root.

**2. Create `loader.ts`** (packages/daemon/src/infrastructure/config/loader.ts):

Implement the config loading pipeline from doc 24 section 3.5. The full Zod schema lives here in the daemon package (not in @waiaas/core -- the core ConfigSchema is a simplified version; the daemon needs the full 7-section schema).

**DaemonConfigSchema** -- a Zod object with 7 section sub-objects, each containing flat keys with defaults. Copy exactly from doc 24 section 3.5 lines 1037-1111:

```typescript
import { parse } from 'smol-toml';
import { z } from 'zod';
import { readFileSync } from 'node:fs';
import path from 'node:path';

export const DaemonConfigSchema = z.object({
  daemon: z.object({
    port: z.number().int().min(1024).max(65535).default(3100),
    hostname: z.union([z.literal('127.0.0.1'), z.literal('0.0.0.0')]).default('127.0.0.1'),
    log_level: z.enum(['trace', 'debug', 'info', 'warn', 'error']).default('info'),
    log_file: z.string().default('logs/daemon.log'),
    log_max_size: z.string().default('50MB'),
    log_max_files: z.number().int().min(1).max(100).default(5),
    pid_file: z.string().default('daemon.pid'),
    shutdown_timeout: z.number().int().min(5).max(300).default(30),
    dev_mode: z.boolean().default(false),
  }).default({}),
  keystore: z.object({
    argon2_memory: z.number().int().min(32768).max(1048576).default(65536),
    argon2_time: z.number().int().min(1).max(20).default(3),
    argon2_parallelism: z.number().int().min(1).max(16).default(4),
    backup_on_rotate: z.boolean().default(true),
  }).default({}),
  database: z.object({
    path: z.string().default('data/waiaas.db'),
    wal_checkpoint_interval: z.number().int().min(60).max(3600).default(300),
    busy_timeout: z.number().int().min(1000).max(30000).default(5000),
    cache_size: z.number().int().min(2000).max(512000).default(64000),
    mmap_size: z.number().int().min(0).max(1073741824).default(268435456),
  }).default({}),
  rpc: z.object({
    solana_mainnet: z.string().default('https://api.mainnet-beta.solana.com'),
    solana_devnet: z.string().default('https://api.devnet.solana.com'),
    solana_testnet: z.string().default('https://api.testnet.solana.com'),
    solana_ws_mainnet: z.string().default('wss://api.mainnet-beta.solana.com'),
    solana_ws_devnet: z.string().default('wss://api.devnet.solana.com'),
    ethereum_mainnet: z.string().default(''),
    ethereum_sepolia: z.string().default(''),
  }).default({}),
  notifications: z.object({
    enabled: z.boolean().default(false),
    min_channels: z.number().int().min(1).max(10).default(2),
    health_check_interval: z.number().int().min(60).max(3600).default(300),
    log_retention_days: z.number().int().min(7).max(365).default(30),
    dedup_ttl: z.number().int().min(60).max(3600).default(300),
    telegram_bot_token: z.string().default(''),
    telegram_chat_id: z.string().default(''),
    discord_webhook_url: z.string().default(''),
    ntfy_server: z.string().default('https://ntfy.sh'),
    ntfy_topic: z.string().default(''),
  }).default({}),
  security: z.object({
    session_ttl: z.number().int().min(300).max(604800).default(86400),
    jwt_secret: z.string().default(''),
    max_sessions_per_agent: z.number().int().min(1).max(50).default(5),
    max_pending_tx: z.number().int().min(1).max(100).default(10),
    nonce_storage: z.enum(['memory', 'sqlite']).default('memory'),
    nonce_cache_max: z.number().int().min(100).max(10000).default(1000),
    nonce_cache_ttl: z.number().int().min(60).max(600).default(300),
    rate_limit_global_ip_rpm: z.number().int().min(100).max(10000).default(1000),
    rate_limit_session_rpm: z.number().int().min(10).max(5000).default(300),
    rate_limit_tx_rpm: z.number().int().min(1).max(100).default(10),
    cors_origins: z.array(z.string()).default(['http://localhost:3100', 'http://127.0.0.1:3100']),
    auto_stop_consecutive_failures_threshold: z.number().int().min(1).max(20).default(3),
    policy_defaults_delay_seconds: z.number().int().min(60).max(3600).default(300),
    policy_defaults_approval_timeout: z.number().int().min(300).max(86400).default(3600),
    kill_switch_recovery_cooldown: z.number().int().min(600).max(86400).default(1800),
    kill_switch_max_recovery_attempts: z.number().int().min(1).max(10).default(3),
  }).default({}),
  walletconnect: z.object({
    project_id: z.string().default(''),
  }).default({}),
});

export type DaemonConfig = z.infer<typeof DaemonConfigSchema>;
```

**detectNestedSections()**: Walk top-level keys. If any value is an object whose own values are also objects (not arrays), throw an error identifying the nested section. Known sections: daemon, keystore, database, rpc, notifications, security, walletconnect. Unknown top-level key also throws.

**applyEnvOverrides()**: Iterate `process.env` for keys starting with `WAIAAS_`. Skip special keys (`WAIAAS_DATA_DIR`, `WAIAAS_MASTER_PASSWORD`, `WAIAAS_MASTER_PASSWORD_FILE`, `WAIAAS_DAEMON_HOSTNAME`). For each: strip prefix, lowercase, split on `_`, first part = section, rest joined with `_` = field. Handle `WAIAAS_DAEMON_HOSTNAME` specially: maps to `daemon.hostname`. Parse values: `'true'`/`'false'` -> boolean, numeric strings -> number, `[...]` -> JSON array parse, else string.

**loadConfig(dataDir: string): DaemonConfig**: Pipeline:
1. Try to read `{dataDir}/config.toml`. If ENOENT, use empty object (all defaults).
2. Parse with `smol-toml` `parse()`.
3. `detectNestedSections(parsed)`.
4. `applyEnvOverrides(parsed)`.
5. `DaemonConfigSchema.parse(merged)` -- Zod applies defaults and validates.
6. Return typed config.

**3. Create `daemon.ts`** (packages/daemon/src/lifecycle/daemon.ts):

Implement the `DaemonLifecycle` class following doc 28 sections 2-3.

```typescript
export class DaemonLifecycle {
  private isShuttingDown = false;
  private sqlite: Database | null = null;
  private db: BetterSQLite3Database | null = null;
  private keyStore: LocalKeyStore | null = null;
  private workers: BackgroundWorkers | null = null;
  private lockFd = -1;
  private pidPath = '';
  private config: DaemonConfig | null = null;

  async start(dataDir: string, masterPassword: string): Promise<void> { ... }
  async shutdown(signal: string): Promise<void> { ... }
}
```

**start() -- 6-step sequence** (doc 28 section 2.1, simplified for v1.1 -- Steps 4 & 5 are stubs since API server and chain adapters are Phase 50):

Step 1 (5s timeout, fail-fast): **Environment validation + config + flock**
- Load config via `loadConfig(dataDir)`
- Call `acquireDaemonLock(dataDir)` to get exclusive flock
- If lock fails with EWOULDBLOCK/EAGAIN, throw `DAEMON_ALREADY_RUNNING` error
- Write PID to lock file for informational purposes

Step 2 (30s timeout, fail-fast): **Database initialization**
- Call `createDatabase(dbPath)` from 49-01 module
- Call `pushSchema(db)` to ensure all 7 tables exist
- Verify PRAGMAs applied

Step 3 (30s timeout, fail-fast): **Keystore unlock** (v1.1: partial -- no agents to decrypt yet, but validate infrastructure)
- Instantiate `LocalKeyStore` from 49-02 module
- If masterPassword provided and keystore dir has key files, attempt decrypt
- Otherwise, just verify keystore directory exists and is accessible

Step 4 (10s/chain, fail-soft): **Adapter initialization** -- STUB for v1.1
- Log "Adapter initialization deferred to Phase 50"
- Return empty adapter registry placeholder

Step 5 (5s timeout, fail-fast): **HTTP server start** -- STUB for v1.1
- Log "HTTP server deferred to Phase 50"
- (Phase 50 will implement Hono server here)

Step 6 (no timeout, fail-soft): **Background workers + PID**
- Start `BackgroundWorkers` (WAL checkpoint + session cleanup)
- Write PID file at `{dataDir}/{config.daemon.pid_file}`
- Log ready message: `WAIaaS daemon ready (PID: {pid})`

**Overall 90s timeout wrapper** using `withTimeout()` utility:
```typescript
function withTimeout<T>(promise: Promise<T>, ms: number, errorCode: string): Promise<T> {
  return Promise.race([
    promise,
    new Promise<never>((_, reject) =>
      setTimeout(() => reject(new WAIaaSError(errorCode, `Timeout after ${ms}ms`)), ms)
    ),
  ]);
}
```

**shutdown() -- 10-step graceful cascade** (doc 28 section 3):

Guard against multiple signals with `isShuttingDown` flag. Start a 30s force-exit timer (configurable via `config.daemon.shutdown_timeout`). `forceTimeout.unref()` so it does not prevent exit.

Steps 1-6 are simplified for v1.1 (no HTTP server, no in-flight signing):
1. Set `isShuttingDown = true`, start force timer, log signal
2-4. HTTP server close -- STUB (Phase 50)
5. In-flight signing -- STUB (Phase 50)
6. Pending queue persistence -- STUB (Phase 50)

Steps 7-10 (active in v1.1):
7. `workers.stopAll()` -- clear all intervals, wait for in-progress handlers
8. `sqlite.pragma('wal_checkpoint(TRUNCATE)')` -- full WAL merge
9. `keyStore.lockAll()` -- sodium_memzero all guarded buffers
10. `sqlite.close()`, unlink PID file, `closeSync(lockFd)` if >= 0, cancel force timer, `process.exit(0)`

**acquireDaemonLock(dataDir)**:
- Open `{dataDir}/daemon.lock` with 'w' flag
- On Unix: use `flock` exclusive non-blocking via a small native binding or `fs-ext` or implement using `fcntl` via node:fs
- Implementation note: Node.js does not have built-in flock. Options:
  a) Use `proper-lockfile` npm package (pure JS, uses rename-based lock, simpler but less robust)
  b) Use `fd-lock` npm package (native flock binding, lightweight)
  c) Implement using `node:child_process` calling `flock` CLI (Unix only)
  d) Use port binding as fallback (try to open a Unix socket on a known path)

  **Decision for v1.1:** Use `proper-lockfile` for cross-platform support. It uses stale lock detection and retry mechanisms. Add to dependencies. If not available, fall back to attempting port bind detection.

  Alternative simpler approach: Write PID to lock file atomically. On start, check if lock file exists and if the PID in it is still running. This is the classic PID-file approach. Combined with the fact that the HTTP port bind (Phase 50) provides a secondary guard, this is acceptable for v1.1. However, per the design doc's preference for flock, prefer `proper-lockfile`.

  Add `proper-lockfile` ^4.4.0 to daemon dependencies.

**4. Create `signal-handler.ts`** (packages/daemon/src/lifecycle/signal-handler.ts):

```typescript
export function registerSignalHandlers(daemon: DaemonLifecycle): void {
  process.on('SIGINT', () => daemon.shutdown('SIGINT'));
  process.on('SIGTERM', () => daemon.shutdown('SIGTERM'));
  if (process.platform === 'win32') {
    process.on('SIGBREAK', () => daemon.shutdown('SIGBREAK'));
  }
  process.on('uncaughtException', (err) => {
    console.error('Uncaught exception:', err);
    daemon.shutdown('uncaughtException').finally(() => process.exit(1));
  });
  process.on('unhandledRejection', (reason) => {
    console.error('Unhandled rejection:', reason);
    daemon.shutdown('unhandledRejection').finally(() => process.exit(1));
  });
}
```

**5. Create `workers.ts`** (packages/daemon/src/lifecycle/workers.ts):

```typescript
interface WorkerRegistration {
  name: string;
  interval: number; // ms
  handler: () => void | Promise<void>;
}

export class BackgroundWorkers {
  private timers: Map<string, NodeJS.Timeout> = new Map();
  private running: Map<string, boolean> = new Map();

  register(name: string, opts: { interval: number; handler: () => void | Promise<void> }): void { ... }

  startAll(): void {
    for (const [name, registration] of this.registrations) {
      const timer = setInterval(async () => {
        if (this.running.get(name)) return; // skip if previous run still active
        this.running.set(name, true);
        try {
          await registration.handler();
        } catch (err) {
          console.error(`Worker ${name} error:`, err);
        } finally {
          this.running.set(name, false);
        }
      }, registration.interval);
      timer.unref(); // don't prevent process exit
      this.timers.set(name, timer);
    }
  }

  async stopAll(): Promise<void> {
    for (const [name, timer] of this.timers) {
      clearInterval(timer);
    }
    this.timers.clear();
    // Wait for any in-progress handlers to complete (max 5s)
    const deadline = Date.now() + 5000;
    while ([...this.running.values()].some(Boolean) && Date.now() < deadline) {
      await new Promise(r => setTimeout(r, 100));
    }
  }
}
```

Register two workers in DaemonLifecycle.start() Step 6:
- `wal-checkpoint`: interval = `config.database.wal_checkpoint_interval * 1000` (default 300s = 5 min), handler = `sqlite.pragma('wal_checkpoint(PASSIVE)')`
- `session-cleanup`: interval = 60_000 (1 min), handler = `DELETE FROM sessions WHERE expires_at < unixepoch() AND revoked_at IS NULL` (simple SQL via db.run)

**6. Create barrel exports:**
- `packages/daemon/src/infrastructure/config/index.ts` -- re-export loadConfig, DaemonConfigSchema, DaemonConfig
- `packages/daemon/src/lifecycle/index.ts` -- re-export DaemonLifecycle, registerSignalHandlers, BackgroundWorkers

**7. Update `packages/daemon/src/index.ts`:**
- Re-export the config loader and lifecycle modules
- Export a top-level `startDaemon(dataDir, masterPassword)` convenience function
  </action>
  <verify>
- `pnpm install` succeeds with smol-toml and proper-lockfile
- `pnpm build --filter=@waiaas/daemon` compiles without TypeScript errors
- DaemonLifecycle class references createDatabase from 49-01 and LocalKeyStore from 49-02 correctly
  </verify>
  <done>
- loadConfig() parses config.toml with smol-toml, detects nested sections, applies env overrides, validates with Zod
- DaemonConfigSchema defines all 7 sections with correct flat keys, types, ranges, and defaults
- DaemonLifecycle.start() implements 6-step startup with per-step timeouts and 90s overall cap
- DaemonLifecycle.shutdown() implements 10-step graceful cascade with WAL checkpoint, keystore lock, PID cleanup
- acquireDaemonLock() prevents second instance via file locking
- registerSignalHandlers() wires SIGINT/SIGTERM/SIGBREAK to shutdown
- BackgroundWorkers registers WAL checkpoint (5-min) and session cleanup (1-min) periodic tasks
- All code compiles and builds successfully
  </done>
</task>

<task type="auto">
  <name>Task 2: Config + lifecycle tests -- TOML parsing, env override, flock contention, startup/shutdown, workers</name>
  <files>
    packages/daemon/src/__tests__/config-loader.test.ts
    packages/daemon/src/__tests__/lifecycle.test.ts
  </files>
  <action>
Create two test files covering the config loader and lifecycle modules.

**A. config-loader.test.ts** -- tests for the config loading pipeline:

1. **TOML parsing tests:**
   - loadConfig with a valid config.toml file returns parsed config
   - loadConfig with missing config.toml returns all defaults (no file = all defaults)
   - loadConfig with empty config.toml returns all defaults
   - loadConfig with partial config (only [daemon] section) fills other sections with defaults
   - Parsed config has correct default values: port=3100, hostname='127.0.0.1', log_level='info', session_ttl=86400

2. **Nested section rejection tests:**
   - config.toml with `[rpc.solana]` nested section throws explicit error mentioning "flattened keys"
   - config.toml with `[security.auto_stop]` nested section throws
   - config.toml with unknown section `[unknown]` throws error listing allowed sections

3. **Environment variable override tests:**
   - Set `WAIAAS_DAEMON_PORT=4000` -> config.daemon.port === 4000
   - Set `WAIAAS_DAEMON_LOG_LEVEL=debug` -> config.daemon.log_level === 'debug'
   - Set `WAIAAS_RPC_SOLANA_MAINNET=https://custom.rpc.com` -> config.rpc.solana_mainnet === 'https://custom.rpc.com'
   - Set `WAIAAS_SECURITY_SESSION_TTL=7200` -> config.security.session_ttl === 7200 (number, not string)
   - Set `WAIAAS_NOTIFICATIONS_ENABLED=true` -> config.notifications.enabled === true (boolean, not string)
   - Env override takes priority over toml value: toml has port=3100, env has WAIAAS_DAEMON_PORT=5000, result is 5000
   - Special keys (WAIAAS_DATA_DIR, WAIAAS_MASTER_PASSWORD) are NOT applied to config

4. **Zod validation tests:**
   - port below 1024 fails validation
   - port above 65535 fails validation
   - hostname 'evil.com' fails validation (only '127.0.0.1' or '0.0.0.0' allowed)
   - log_level 'verbose' (not in enum) fails validation
   - shutdown_timeout below 5 fails validation

5. **parseEnvValue tests:**
   - 'true' -> true (boolean)
   - 'false' -> false (boolean)
   - '3100' -> 3100 (number)
   - '["a","b"]' -> ['a', 'b'] (JSON array)
   - 'hello' -> 'hello' (string)

**Test setup for config tests:**
- Create a temp directory with a config.toml file for each test
- Use `process.env` manipulation (save/restore in beforeEach/afterEach) for env override tests
- Clean up temp files in afterAll

**B. lifecycle.test.ts** -- tests for daemon lifecycle, flock, PID, workers:

1. **flock / lock contention tests:**
   - acquireDaemonLock() succeeds on first call (returns fd or lock handle)
   - acquireDaemonLock() on same dataDir fails with ALREADY_RUNNING or lock contention error when first lock is held
   - After releasing first lock, second acquireDaemonLock() succeeds
   - Lock file contains PID number

2. **PID file tests:**
   - After start(), PID file exists at `{dataDir}/{config.daemon.pid_file}`
   - PID file contains `process.pid` as text
   - After shutdown(), PID file is deleted

3. **Startup timeout tests:**
   - withTimeout() resolves normally for fast promise
   - withTimeout() rejects with timeout error for slow promise (>ms limit)
   - withTimeout() error includes the provided errorCode

4. **BackgroundWorkers tests:**
   - Worker handler is called at the registered interval (use vi.useFakeTimers)
   - stopAll() clears all intervals
   - Worker that throws does not crash -- error is caught, next interval still fires
   - Worker that is still running when next interval fires is skipped (no overlap)
   - stopAll() waits for in-progress handler to complete (up to 5s)

5. **Shutdown sequence tests:**
   - shutdown() sets isShuttingDown to true
   - Double shutdown call (second signal) is ignored (idempotent)
   - shutdown() calls WAL checkpoint(TRUNCATE) on the SQLite connection
   - shutdown() calls keyStore.lockAll() for memory zeroing
   - shutdown() deletes the PID file

6. **Signal handler tests:**
   - registerSignalHandlers() registers SIGINT and SIGTERM listeners
   - On Windows (mock process.platform), SIGBREAK listener is also registered

**Test setup for lifecycle tests:**
- Use temp directories for dataDir, lock files, PID files
- For BackgroundWorkers tests, use `vi.useFakeTimers()` and `vi.advanceTimersByTime()`
- For shutdown tests, mock the sqlite/keyStore objects to verify method calls
- Clean up temp directories in afterAll
  </action>
  <verify>
- `pnpm test --filter=@waiaas/daemon` passes all config-loader and lifecycle tests
- Config tests verify TOML parsing, env override priority, nested section rejection, Zod validation
- Lifecycle tests verify flock contention, PID management, worker scheduling, shutdown sequence
  </verify>
  <done>
- Config TOML parsing returns correct defaults when file is missing or empty
- Nested sections like [rpc.solana] are rejected with clear error
- Environment variables override toml values with correct type coercion (string -> number/boolean)
- Zod validation rejects out-of-range values (port, hostname, log_level)
- flock prevents second daemon instance with explicit ALREADY_RUNNING error
- PID file is created on start and deleted on shutdown
- BackgroundWorkers fire at registered intervals and handle errors gracefully
- Shutdown sequence calls WAL checkpoint, keystore lock, and cleans up resources
- At least 25 test assertions pass across both test files
  </done>
</task>

</tasks>

<verification>
1. `pnpm build --filter=@waiaas/daemon` succeeds
2. `pnpm test --filter=@waiaas/daemon` passes all config-loader and lifecycle tests
3. loadConfig() pipeline: smol-toml parse -> nested detection -> env override -> Zod validate
4. DaemonLifecycle.start() runs 6 steps with timeouts, DaemonLifecycle.shutdown() runs 10 steps
5. flock prevents concurrent daemon instances
6. BackgroundWorkers run periodic WAL checkpoint and session cleanup
</verification>

<success_criteria>
- config.toml loaded by smol-toml with all 7 sections validated by DaemonConfigSchema Zod schema
- Environment variables WAIAAS_{SECTION}_{KEY} override toml values with correct priority (env > toml > default)
- Nested TOML sections detected and rejected with explicit error
- 6-step startup with per-step timeouts (5s/30s/30s/10s/5s/none) and 90s overall cap
- 10-step graceful shutdown with WAL checkpoint(TRUNCATE), keystore lock (sodium_memzero), PID cleanup
- flock exclusive lock prevents multiple daemon instances
- BackgroundWorkers run WAL checkpoint (5-min) and session cleanup (1-min) periodically
- Comprehensive test suite covering config parsing, env overrides, flock, startup/shutdown, workers
</success_criteria>

<output>
After completion, create `.planning/phases/49-daemon-infra/49-03-SUMMARY.md`
</output>
